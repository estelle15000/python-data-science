{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation contrainte avec multiplicateurs de Lagrange et autograd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'optimisation contrainte est courante dans la résolution de problèmes d'ingénierie. Un exemple prototypique (de Greenberg, Advanced Engineering Mathematics, Ch 13.7) est de trouver le point sur un plan qui est le plus proche de l'origine. \n",
    "\n",
    "Le plan est défini par l'équation 2x−y+z=3, et on cherche à minimiser x2+y2+z2 sous la contrainte d'égalité définie par le plan. scipy.optimize.minimize fournit une interface assez pratique pour résoudre un problème comme celui-ci, et illustré ici.\n",
    "\n",
    "Lien : https://kitchingroup.cheme.cmu.edu/blog/2018/11/03/Constrained-optimization-with-Lagrange-multipliers-and-autograd/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " message: Optimization terminated successfully\n",
       " success: True\n",
       "  status: 0\n",
       "     fun: 1.5\n",
       "       x: [ 1.000e+00 -5.000e-01  5.000e-01]\n",
       "     nit: 1\n",
       "     jac: [ 2.000e+00 -1.000e+00  1.000e+00]\n",
       "    nfev: 4\n",
       "    njev: 1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def objective(X):\n",
    "    x, y, z = X\n",
    "    return x**2 + y**2 + z**2\n",
    "\n",
    "def eq(X):\n",
    "    x, y, z = X\n",
    "    return 2 * x - y + z - 3\n",
    "\n",
    "sol = minimize(objective, [1, -0.5, 0.5], constraints={'type': 'eq', 'fun': eq})\n",
    "sol"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'aime beaucoup la fonction de minimisation, bien que je ne sois pas fou de la façon dont les contraintes sont fournies. L'alternative était qu'il y avait un argument pour les contraintes d'égalité et un autre pour les contraintes d'inégalité. Analogues aux fonctions d'événement scipy.integrate.solve_ivp, elles auraient également pu utiliser des attributs de fonction.\n",
    "\n",
    "Parfois, il peut être souhaitable de revenir à l'essentiel, surtout si vous n'êtes pas au courant de la fonction de minimisation ou si vous soupçonnez qu'elle ne fonctionne pas correctement et que vous souhaitez une réponse indépendante. \n",
    "Ensuite, nous regardons comment construire ce problème d'optimisation sous contrainte en utilisant des multiplicateurs de Lagrange. Cela convertit le problème en un problème d'optimisation augmenté sans contrainte sur lequel nous pouvons utiliser fsolve. L'essentiel de cette méthode est que nous formulons un nouveau problème :\n",
    "\n",
    "F(X)=f(X)−λg(X)\n",
    "\n",
    "puis résolvez les équations résultantes simultanées :\n",
    "\n",
    "Fx(X)=Fy(X)=Fz(X)=g(X)=0\n",
    "où Fx est la dérivée de f∗ par rapport à x, et g(X) est la contrainte d'égalité écrite telle qu'elle soit égale à zéro. \n",
    "\n",
    "Puisque nous nous retrouvons avec quatre équations égales à zéro, nous pouvons simplement utiliser fsolve pour obtenir la solution. Il y a de nombreuses années, j'ai utilisé une approximation aux différences finies des dérivées. Aujourd'hui, nous utilisons autograd pour obtenir les dérivées souhaitées. C'est ici.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is at (1.0, -0.5, 0.5)\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "def F(L):\n",
    "    'Augmented Lagrange function'\n",
    "    x, y, z, _lambda = L\n",
    "    return objective([x, y, z]) - _lambda * eq([x, y, z])\n",
    "\n",
    "# Gradients of the Lagrange function\n",
    "dfdL = grad(F, 0)\n",
    "\n",
    "# Find L that returns all zeros in this function.\n",
    "def obj(L):\n",
    "    x, y, z, _lambda = L\n",
    "    dFdx, dFdy, dFdz, dFdlam = dfdL(L)\n",
    "    return [dFdx, dFdy, dFdz, eq([x, y, z])]\n",
    "\n",
    "from scipy.optimize import fsolve\n",
    "x, y, z, _lam = fsolve(obj, [0.0, 0.0, 0.0, 1.0])\n",
    "print(f'The answer is at {x, y, z}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est la même réponse que précédemment. Notez que nous nous sommes toujours appuyés sur un solveur de boîte noire à l'intérieur de fsolve (au lieu de minimiser à l'intérieur), mais il pourrait être plus clair quel problème nous résolvons (par exemple, trouver des zéros). Cela demande un peu plus de travail pour configurer cela, car nous devons construire la fonction augmentée, mais autograd rend assez pratique la configuration de la fonction objectif finale que nous voulons résoudre.\n",
    "\n",
    "Comment savons-nous que nous sommes au minimum ? Nous pouvons vérifier que le hessien est défini positif dans la fonction d'origine que nous voulions minimiser. Vous pouvez voir ici que le tableau est défini positif, par ex. toutes les valeurs propres sont positives. autograd rend cela facile aussi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0.],\n",
       "       [0., 2., 0.],\n",
       "       [0., 0., 2.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autograd import hessian\n",
    "h = hessian(objective, 0)\n",
    "h(np.array([x, y, z]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dans le cas où il ne ressort pas de cette structure que les valeurs propres sont toutes positives, nous les calculons ici :\n",
    "\n",
    "np.linalg.eig(h(np.array([x, y, z])))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In summary, autograd continues to enable advanced engineering problems to be solved.\n",
    "\n",
    "Copyright (C) 2018 by John Kitchin. See the License for information about copying."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
